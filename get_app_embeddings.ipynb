{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_app_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVEoOTsdapBM"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, json\n",
        "from glob import glob\n",
        "import time\n",
        "import re\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pmAKxCj1iq_"
      },
      "source": [
        "!pip install pattern\n",
        "!pip install wordfreq\n",
        "!pip install simplemma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poKBLVJuLZNm"
      },
      "source": [
        "import simplemma\n",
        "from wordfreq import word_frequency\n",
        "from pattern.web import plaintext"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqjKBfy2bjbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f26054-3151-4634-a51c-89d7137d8acf"
      },
      "source": [
        "!mkdir /content/hackfinal\n",
        "%cd /content/hackfinal"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackfinal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdrE2Wc2KGpQ"
      },
      "source": [
        "import json\n",
        "# with open(\"/content/hackfinal/apps_info.json\", 'r') as f:\n",
        "with open(\"/content/drive/MyDrive/colab/hackfinal/apps_info.json\", 'r') as f:\n",
        "    info = json.load(f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e5bcgVsAQc7"
      },
      "source": [
        "regex_tokenize_ru = re.compile(\"[^а-я]\")\n",
        "regex_tokenize_en = re.compile(\"[^a-z]\")\n",
        "# regex_tokenize = re.compile(\"[^\\w]\")\n",
        "langdata_en = simplemma.load_data('en')\n",
        "langdata_ru = simplemma.load_data('ru')\n",
        "\n",
        "def tokenize_ru(descr_raw):\n",
        "    descr_plaintext = plaintext(descr_raw).lower()\n",
        "    words = regex_tokenize_ru.sub(\" \", descr_plaintext).split()\n",
        "    words_lem = list(map(lambda w: simplemma.lemmatize(w, langdata_ru), words))\n",
        "    return words_lem\n",
        "\n",
        "def tokenize_en(descr_raw):\n",
        "    descr_plaintext = plaintext(descr_raw).lower()\n",
        "    words = regex_tokenize_en.sub(\" \", descr_plaintext).split()\n",
        "    words_lem = list(map(lambda w: simplemma.lemmatize(w, langdata_en), words))\n",
        "    return words_lem"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ql7L3l8Acsk",
        "outputId": "1b520c5f-efcd-4310-d45c-7a5510edd4c9"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "keys_list = sorted(list(info.keys()))\n",
        "words_counter_ru = Counter()\n",
        "words_counter_en = Counter()\n",
        "sentences_ru = []\n",
        "sentences_en = []\n",
        "skipped = 0\n",
        "min_sentence_len = 2\n",
        "for bundle in tqdm(keys_list):\n",
        "    app_info = info[bundle]\n",
        "    descr = app_info['description']\n",
        "    descr = descr.replace('[', ' ')\n",
        "    descr_words_ru = tokenize_ru(descr)\n",
        "    descr_words_en = tokenize_en(descr)\n",
        "    if len(descr_words_ru) > len(descr_words_en):\n",
        "        if len(descr_words_ru) >= min_sentence_len:\n",
        "            sentences_ru.append([bundle, descr_words_ru])\n",
        "        else:\n",
        "            skipped += 1\n",
        "        words_counter_ru.update(descr_words_ru)\n",
        "    else:\n",
        "        if len(descr_words_en) >= min_sentence_len:\n",
        "            sentences_en.append([bundle, descr_words_en])\n",
        "        else:\n",
        "            skipped += 1\n",
        "        words_counter_en.update(descr_words_en)\n",
        "print(len(words_counter_ru), len(words_counter_en))\n",
        "print(\"skipped:\", skipped)\n",
        "\n",
        "words_counter_ru_view = list(words_counter_ru.items())\n",
        "words_counter_ru_view.sort(key=lambda x: (x[1], x[0]), reverse=True)\n",
        "words_counter_en_view = list(words_counter_en.items())\n",
        "words_counter_en_view.sort(key=lambda x: (x[1], x[0]), reverse=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70392/70392 [01:55<00:00, 607.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "89282 88218\n",
            "skipped: 378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0wD6MlB0gw4"
      },
      "source": [
        "with open(\"/content/hackfinal/sentences_ru.txt\", \"w\") as f:\n",
        "    for (bundle, sent) in sentences_ru:\n",
        "        f.write(bundle + \" \" + \" \".join(sent)+\"\\n\")\n",
        "\n",
        "with open(\"/content/hackfinal/sentences_en.txt\", \"w\") as f:\n",
        "    for (bundle, sent) in sentences_en:\n",
        "        f.write(bundle + \" \" + \" \".join(sent)+\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsHn4IGsG4KF"
      },
      "source": [
        "with open(\"/content/hackfinal/words_ru.txt\", \"w\") as f:\n",
        "    for (k,v) in words_counter_ru_view:\n",
        "        f.write(k+'\\n')\n",
        "\n",
        "with open(\"/content/hackfinal/words_en.txt\", \"w\") as f:\n",
        "    for (k,v) in words_counter_en_view:\n",
        "        f.write(k+'\\n')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHf-nKFH2zl9"
      },
      "source": [
        "with open(\"/content/hackfinal/freqs_ru.txt\", \"w\") as f:\n",
        "    for (word, _) in words_counter_ru_view:\n",
        "        freq = word_frequency(word, 'ru')\n",
        "        f.write(word + \" \" + str(freq) + \"\\n\")\n",
        "\n",
        "with open(\"/content/hackfinal/freqs_en.txt\", \"w\") as f:\n",
        "    for (word, _) in words_counter_en_view:\n",
        "        freq = word_frequency(word, 'en')\n",
        "        f.write(word + \" \" + str(freq) + \"\\n\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtlWfbAaPzRA",
        "outputId": "04df03ad-fc5f-409c-9f94-a86850080d18"
      },
      "source": [
        "%cd /content/hackfinal"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackfinal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR9mgVCCPZGS"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!sudo pip2 install .\n",
        "!make"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWBz7GljPcJh"
      },
      "source": [
        "!cp /content/drive/MyDrive/colab/hackfinal/cc.en.300.bin /content/hackfinal/fastText/\n",
        "!cp /content/drive/MyDrive/colab/hackfinal/cc.ru.300.bin /content/hackfinal/fastText/"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWv8JI7UPA0d",
        "outputId": "3eee8c6d-6f2a-4102-e314-6644c2ee67b3"
      },
      "source": [
        "%cd fastText\n",
        "!python2 download_model.py ru\n",
        "!python2 download_model.py en"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'fastText'\n",
            "/content/hackfinal/fastText\n",
            "File exists. Use --overwrite to download anyway.\n",
            "File exists. Use --overwrite to download anyway.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFNy_qwwQo7N",
        "outputId": "96870d31-4622-4232-a1ed-c0c6d3dc4eb0"
      },
      "source": [
        "%cd /content/hackfinal/fastText\n",
        "t1 = time.time()\n",
        "!./fasttext print-word-vectors cc.ru.300.bin < /content/hackfinal/words_ru.txt > /content/hackfinal/words_ru_vectors.txt\n",
        "t2 = time.time()\n",
        "print(t2-t1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackfinal/fastText\n",
            "tcmalloc: large alloc 4800004096 bytes == 0x55f442a70000 @  0x7f4f25d4d887 0x55f42e6987d3 0x55f42e6a7199 0x55f42e6a76dd 0x55f42e6b27b6 0x55f42e66ca9b 0x7f4f24deabf7 0x55f42e66cb1a\n",
            "tcmalloc: large alloc 2400002048 bytes == 0x55f560c14000 @  0x7f4f25d4d887 0x55f42e6987d3 0x55f42e6a71e8 0x55f42e6a76dd 0x55f42e6b27b6 0x55f42e66ca9b 0x7f4f24deabf7 0x55f42e66cb1a\n",
            "123.30860137939453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnthmhgFQ7o-",
        "outputId": "11e78341-54c3-4e80-a41f-e2bb51fc7ae4"
      },
      "source": [
        "%cd /content/hackfinal/fastText\n",
        "t1 = time.time()\n",
        "!./fasttext print-word-vectors cc.en.300.bin < /content/hackfinal/words_en.txt > /content/hackfinal/words_en_vectors.txt\n",
        "t2 = time.time()\n",
        "print(t2-t1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackfinal/fastText\n",
            "tcmalloc: large alloc 4800004096 bytes == 0x55ef1bba4000 @  0x7fbc296d1887 0x55ef0835b7d3 0x55ef0836a199 0x55ef0836a6dd 0x55ef083757b6 0x55ef0832fa9b 0x7fbc2876ebf7 0x55ef0832fb1a\n",
            "tcmalloc: large alloc 2400002048 bytes == 0x55f039d48000 @  0x7fbc296d1887 0x55ef0835b7d3 0x55ef0836a1e8 0x55ef0836a6dd 0x55ef083757b6 0x55ef0832fa9b 0x7fbc2876ebf7 0x55ef0832fb1a\n",
            "121.10628819465637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBikcc6Nh6Fc",
        "outputId": "5cc7930c-e75a-49c0-e90b-a0377203a98a"
      },
      "source": [
        "%cd /content/hackfinal\n",
        "!git clone https://github.com/PrincetonML/SIF.git\n",
        "%cd SIF\n",
        "!pip2 install -r /content/hackfinal/SIF/requirements.txt"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackfinal\n",
            "Cloning into 'SIF'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 2.80 MiB | 11.50 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "/content/hackfinal/SIF\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from -r /content/hackfinal/SIF/requirements.txt (line 1)) (1.16.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (from -r /content/hackfinal/SIF/requirements.txt (line 2)) (1.2.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (from -r /content/hackfinal/SIF/requirements.txt (line 3)) (0.0)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python2.7/dist-packages (from -r /content/hackfinal/SIF/requirements.txt (line 4)) (1.0.5)\n",
            "Collecting lasagne\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/bf/4b2336e4dbc8c8859c4dd81b1cff18eef2066b4973a1bd2b0ca2e5435f35/Lasagne-0.1.tar.gz (125kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 14.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn->-r /content/hackfinal/SIF/requirements.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from theano->-r /content/hackfinal/SIF/requirements.txt (line 4)) (1.15.0)\n",
            "Building wheels for collected packages: lasagne\n",
            "  Building wheel for lasagne (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lasagne: filename=Lasagne-0.1-cp27-none-any.whl size=79275 sha256=194ed0137ee9e6fb89734cd6844138808c15962e20454fd749ce7db8c3e8cbda\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/8e/31/b4cae7e5507f8582e77d7f5cf2815be8820ccacfa0519ca60c\n",
            "Successfully built lasagne\n",
            "Installing collected packages: lasagne\n",
            "Successfully installed lasagne-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjPVamS1UMAR"
      },
      "source": [
        "# write script for our case of input and output format:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FaWEx3DLuZP",
        "outputId": "7ea9d39c-0d87-4464-eb1d-cf79ac89237f"
      },
      "source": [
        "%%writefile /content/hackfinal/SIF/src/sif_embedding_my.py\n",
        "import sys\n",
        "sys.path.append('../src')\n",
        "import data_io, params, SIF_embedding\n",
        "\n",
        "# input\n",
        "wordfile = sys.argv[1] # word vector file, can be downloaded from GloVe website\n",
        "weightfile = sys.argv[2] # each line is a word and its frequency\n",
        "weightpara = 1e-3 # the parameter in the SIF weighting scheme, usually in the range [3e-5, 3e-3]\n",
        "rmpc = 1 # number of principal components to remove in SIF weighting scheme\n",
        "sentences_file = sys.argv[3]\n",
        "bundles = []\n",
        "sentences = []\n",
        "with open(sentences_file, \"r\") as f:\n",
        "    for line in f:\n",
        "        spl = line.rstrip().split(' ')\n",
        "        bundle = spl[0]\n",
        "        sentence = ' '.join(spl[1:])\n",
        "        bundles.append(bundle)\n",
        "        sentences.append(sentence)\n",
        "\n",
        "# load word vectors\n",
        "(words, We) = data_io.getWordmap(wordfile)\n",
        "# load word weights\n",
        "word2weight = data_io.getWordWeight(weightfile, weightpara) # word2weight['str'] is the weight for the word 'str'\n",
        "weight4ind = data_io.getWeight(words, word2weight) # weight4ind[i] is the weight for the i-th word\n",
        "# load sentences\n",
        "x, m = data_io.sentences2idx(sentences, words) # x is the array of word indices, m is the binary mask indicating whether there is a word in that location\n",
        "w = data_io.seq2weight(x, m, weight4ind) # get word weights\n",
        "\n",
        "# set parameters\n",
        "params = params.params()\n",
        "params.rmpc = rmpc\n",
        "# get SIF embedding\n",
        "embedding = SIF_embedding.SIF_embedding(We, x, w, params) # embedding[i,:] is the embedding for sentence i\n",
        "\n",
        "for (bundle, emb) in zip(bundles, embedding):\n",
        "    print bundle + \" \" + \" \".join(list(map(str, emb)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/hackfinal/SIF/src/sif_embedding_my.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raIbd2SxUJeT"
      },
      "source": [
        "# fix one bug in original repo source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K8zSuAzMdiw",
        "outputId": "5b4c0ee4-a983-4b1c-c113-bc0fb4074aa2"
      },
      "source": [
        "%%writefile /content/hackfinal/SIF/src/data_io.py\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tree import tree\n",
        "#from theano import config\n",
        "\n",
        "def getWordmap(textfile):\n",
        "    words={}\n",
        "    We = []\n",
        "    f = open(textfile,'r')\n",
        "    lines = f.readlines()\n",
        "    for (n,i) in enumerate(lines):\n",
        "        i=i.split()\n",
        "        j = 1\n",
        "        v = []\n",
        "        while j < len(i):\n",
        "            v.append(float(i[j]))\n",
        "            j += 1\n",
        "        words[i[0]]=n\n",
        "        We.append(v)\n",
        "    return (words, np.array(We))\n",
        "\n",
        "def prepare_data(list_of_seqs):\n",
        "    lengths = [len(s) for s in list_of_seqs]\n",
        "    n_samples = len(list_of_seqs)\n",
        "    maxlen = np.max(lengths)\n",
        "    x = np.zeros((n_samples, maxlen)).astype('int32')\n",
        "    x_mask = np.zeros((n_samples, maxlen)).astype('float32')\n",
        "    for idx, s in enumerate(list_of_seqs):\n",
        "        x[idx, :lengths[idx]] = s\n",
        "        x_mask[idx, :lengths[idx]] = 1.\n",
        "    x_mask = np.asarray(x_mask, dtype='float32')\n",
        "    return x, x_mask\n",
        "\n",
        "def lookupIDX(words,w):\n",
        "    w = w.lower()\n",
        "    if len(w) > 1 and w[0] == '#':\n",
        "        w = w.replace(\"#\",\"\")\n",
        "    if w in words:\n",
        "        return words[w]\n",
        "    elif 'UUUNKKK' in words:\n",
        "        return words['UUUNKKK']\n",
        "    else:\n",
        "        return len(words) - 1\n",
        "\n",
        "def getSeq(p1,words):\n",
        "    p1 = p1.split()\n",
        "    X1 = []\n",
        "    for i in p1:\n",
        "        X1.append(lookupIDX(words,i))\n",
        "    return X1\n",
        "\n",
        "def getSeqs(p1,p2,words):\n",
        "    p1 = p1.split()\n",
        "    p2 = p2.split()\n",
        "    X1 = []\n",
        "    X2 = []\n",
        "    for i in p1:\n",
        "        X1.append(lookupIDX(words,i))\n",
        "    for i in p2:\n",
        "        X2.append(lookupIDX(words,i))\n",
        "    return X1, X2\n",
        "\n",
        "def get_minibatches_idx(n, minibatch_size, shuffle=False):\n",
        "    idx_list = np.arange(n, dtype=\"int32\")\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx_list)\n",
        "\n",
        "    minibatches = []\n",
        "    minibatch_start = 0\n",
        "    for i in range(n // minibatch_size):\n",
        "        minibatches.append(idx_list[minibatch_start:\n",
        "        minibatch_start + minibatch_size])\n",
        "        minibatch_start += minibatch_size\n",
        "\n",
        "    if (minibatch_start != n):\n",
        "        minibatches.append(idx_list[minibatch_start:])\n",
        "\n",
        "    return zip(range(len(minibatches)), minibatches)\n",
        "\n",
        "def getSimEntDataset(f,words,task):\n",
        "    data = open(f,'r')\n",
        "    lines = data.readlines()\n",
        "    examples = []\n",
        "    for i in lines:\n",
        "        i=i.strip()\n",
        "        if(len(i) > 0):\n",
        "            i=i.split('\\t')\n",
        "            if len(i) == 3:\n",
        "                if task == \"sim\":\n",
        "                    e = (tree(i[0], words), tree(i[1], words), float(i[2]))\n",
        "                    examples.append(e)\n",
        "                elif task == \"ent\":\n",
        "                    e = (tree(i[0], words), tree(i[1], words), i[2])\n",
        "                    examples.append(e)\n",
        "                else:\n",
        "                    raise ValueError('Params.traintype not set correctly.')\n",
        "\n",
        "            else:\n",
        "                print(i)\n",
        "    return examples\n",
        "\n",
        "def getSentimentDataset(f,words):\n",
        "    data = open(f,'r')\n",
        "    lines = data.readlines()\n",
        "    examples = []\n",
        "    for i in lines:\n",
        "        i=i.strip()\n",
        "        if(len(i) > 0):\n",
        "            i=i.split('\\t')\n",
        "            if len(i) == 2:\n",
        "                e = (tree(i[0], words), i[1])\n",
        "                examples.append(e)\n",
        "            else:\n",
        "                print(i)\n",
        "    return examples\n",
        "\n",
        "def getDataSim(batch, nout):\n",
        "    g1 = []\n",
        "    g2 = []\n",
        "    for i in batch:\n",
        "        g1.append(i[0].embeddings)\n",
        "        g2.append(i[1].embeddings)\n",
        "\n",
        "    g1x, g1mask = prepare_data(g1)\n",
        "    g2x, g2mask = prepare_data(g2)\n",
        "\n",
        "    scores = []\n",
        "    if nout <=0:\n",
        "        return (scores, g1x, g1mask, g2x, g2mask)\n",
        "\n",
        "    for i in batch:\n",
        "        temp = np.zeros(nout)\n",
        "        score = float(i[2])\n",
        "        ceil, fl = int(np.ceil(score)), int(np.floor(score))\n",
        "        if ceil == fl:\n",
        "            temp[fl - 1] = 1\n",
        "        else:\n",
        "            temp[fl - 1] = ceil - score\n",
        "            temp[ceil - 1] = score - fl\n",
        "        scores.append(temp)\n",
        "    scores = np.matrix(scores) + 0.000001\n",
        "    scores = np.asarray(scores, dtype='float32')\n",
        "    return (scores, g1x, g1mask, g2x, g2mask)\n",
        "\n",
        "def getDataEntailment(batch):\n",
        "    g1 = []; g2 = []\n",
        "    for i in batch:\n",
        "        g1.append(i[0].embeddings)\n",
        "        g2.append(i[1].embeddings)\n",
        "\n",
        "    g1x, g1mask = prepare_data(g1)\n",
        "    g2x, g2mask = prepare_data(g2)\n",
        "\n",
        "    scores = []\n",
        "    for i in batch:\n",
        "        temp = np.zeros(3)\n",
        "        label = i[2].strip()\n",
        "        if label == \"CONTRADICTION\":\n",
        "            temp[0]=1\n",
        "        if label == \"NEUTRAL\":\n",
        "            temp[1]=1\n",
        "        if label == \"ENTAILMENT\":\n",
        "            temp[2]=1\n",
        "        scores.append(temp)\n",
        "    scores = np.matrix(scores)+0.000001\n",
        "    scores = np.asarray(scores,dtype='float32')\n",
        "    return (scores,g1x,g1mask,g2x,g2mask)\n",
        "\n",
        "def getDataSentiment(batch):\n",
        "    g1 = []\n",
        "    for i in batch:\n",
        "        g1.append(i[0].embeddings)\n",
        "\n",
        "    g1x, g1mask = prepare_data(g1)\n",
        "\n",
        "    scores = []\n",
        "    for i in batch:\n",
        "        temp = np.zeros(2)\n",
        "        label = i[1].strip()\n",
        "        if label == \"0\":\n",
        "            temp[0]=1\n",
        "        if label == \"1\":\n",
        "            temp[1]=1\n",
        "        scores.append(temp)\n",
        "    scores = np.matrix(scores)+0.000001\n",
        "    scores = np.asarray(scores,dtype='float32')\n",
        "    return (scores,g1x,g1mask)\n",
        "\n",
        "def sentences2idx(sentences, words):\n",
        "    \"\"\"\n",
        "    Given a list of sentences, output array of word indices that can be fed into the algorithms.\n",
        "    :param sentences: a list of sentences\n",
        "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
        "    :return: x1, m1. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location)\n",
        "    \"\"\"\n",
        "    seq1 = []\n",
        "    for i in sentences:\n",
        "        seq1.append(getSeq(i,words))\n",
        "    x1,m1 = prepare_data(seq1)\n",
        "    return x1, m1\n",
        "\n",
        "\n",
        "def sentiment2idx(sentiment_file, words):\n",
        "    \"\"\"\n",
        "    Read sentiment data file, output array of word indices that can be fed into the algorithms.\n",
        "    :param sentiment_file: file name\n",
        "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
        "    :return: x1, m1, golds. x1[i, :] is the word indices in sentence i, m1[i,:] is the mask for sentence i (0 means no word at the location), golds[i] is the label (0 or 1) for sentence i.\n",
        "    \"\"\"\n",
        "    f = open(sentiment_file,'r')\n",
        "    lines = f.readlines()\n",
        "    golds = []\n",
        "    seq1 = []\n",
        "    for i in lines:\n",
        "        i = i.split(\"\\t\")\n",
        "        p1 = i[0]; score = int(i[1]) # score are labels 0 and 1\n",
        "        X1 = getSeq(p1,words)\n",
        "        seq1.append(X1)\n",
        "        golds.append(score)\n",
        "    x1,m1 = prepare_data(seq1)\n",
        "    return x1, m1, golds\n",
        "\n",
        "def sim2idx(sim_file, words):\n",
        "    \"\"\"\n",
        "    Read similarity data file, output array of word indices that can be fed into the algorithms.\n",
        "    :param sim_file: file name\n",
        "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
        "    :return: x1, m1, x2, m2, golds. x1[i, :] is the word indices in the first sentence in pair i, m1[i,:] is the mask for the first sentence in pair i (0 means no word at the location), golds[i] is the score for pair i (float). x2 and m2 are similar to x1 and m2 but for the second sentence in the pair.\n",
        "    \"\"\"\n",
        "    f = open(sim_file,'r')\n",
        "    lines = f.readlines()\n",
        "    golds = []\n",
        "    seq1 = []\n",
        "    seq2 = []\n",
        "    for i in lines:\n",
        "        i = i.split(\"\\t\")\n",
        "        p1 = i[0]; p2 = i[1]; score = float(i[2])\n",
        "        X1, X2 = getSeqs(p1,p2,words)\n",
        "        seq1.append(X1)\n",
        "        seq2.append(X2)\n",
        "        golds.append(score)\n",
        "    x1,m1 = prepare_data(seq1)\n",
        "    x2,m2 = prepare_data(seq2)\n",
        "    return x1, m1, x2, m2, golds\n",
        "\n",
        "def entailment2idx(sim_file, words):\n",
        "    \"\"\"\n",
        "    Read similarity data file, output array of word indices that can be fed into the algorithms.\n",
        "    :param sim_file: file name\n",
        "    :param words: a dictionary, words['str'] is the indices of the word 'str'\n",
        "    :return: x1, m1, x2, m2, golds. x1[i, :] is the word indices in the first sentence in pair i, m1[i,:] is the mask for the first sentence in pair i (0 means no word at the location), golds[i] is the label for pair i (CONTRADICTION NEUTRAL ENTAILMENT). x2 and m2 are similar to x1 and m2 but for the second sentence in the pair.\n",
        "    \"\"\"\n",
        "    f = open(sim_file,'r')\n",
        "    lines = f.readlines()\n",
        "    golds = []\n",
        "    seq1 = []\n",
        "    seq2 = []\n",
        "    for i in lines:\n",
        "        i = i.split(\"\\t\")\n",
        "        p1 = i[0]; p2 = i[1]; score = i[2]\n",
        "        X1, X2 = getSeqs(p1,p2,words)\n",
        "        seq1.append(X1)\n",
        "        seq2.append(X2)\n",
        "        golds.append(score)\n",
        "    x1,m1 = prepare_data(seq1)\n",
        "    x2,m2 = prepare_data(seq2)\n",
        "    return x1, m1, x2, m2, golds\n",
        "\n",
        "def getWordWeight(weightfile, a=1e-3):\n",
        "    if a <=0: # when the parameter makes no sense, use unweighted\n",
        "        a = 1.0\n",
        "\n",
        "    word2weight = {}\n",
        "    with open(weightfile) as f:\n",
        "        lines = f.readlines()\n",
        "    N = 0\n",
        "    for i in lines:\n",
        "        i=i.strip()\n",
        "        if(len(i) > 0):\n",
        "            i=i.split()\n",
        "            if(len(i) == 2):\n",
        "                word2weight[i[0]] = float(i[1])\n",
        "                N += float(i[1])\n",
        "            else:\n",
        "                print(i)\n",
        "    for key, value in word2weight.iteritems():\n",
        "        word2weight[key] = a / (a + value/N)\n",
        "    return word2weight\n",
        "\n",
        "def getWeight(words, word2weight):\n",
        "    weight4ind = {}\n",
        "    for word, ind in words.iteritems():\n",
        "        if word in word2weight:\n",
        "            weight4ind[ind] = word2weight[word]\n",
        "        else:\n",
        "            weight4ind[ind] = 1.0\n",
        "    return weight4ind\n",
        "\n",
        "def seq2weight(seq, mask, weight4ind):\n",
        "    weight = np.zeros(seq.shape).astype('float32')\n",
        "    for i in xrange(seq.shape[0]):\n",
        "        for j in xrange(seq.shape[1]):\n",
        "            if mask[i,j] > 0 and seq[i,j] >= 0:\n",
        "                weight[i,j] = weight4ind[seq[i,j]]\n",
        "    weight = np.asarray(weight, dtype='float32')\n",
        "    return weight\n",
        "\n",
        "def getIDFWeight(wordfile, save_file=''):\n",
        "    def getDataFromFile(f, words):\n",
        "        f = open(f,'r')\n",
        "        lines = f.readlines()\n",
        "        golds = []\n",
        "        seq1 = []\n",
        "        seq2 = []\n",
        "        for i in lines:\n",
        "            i = i.split(\"\\t\")\n",
        "            p1 = i[0]; p2 = i[1]; score = float(i[2])\n",
        "            X1, X2 = getSeqs(p1,p2,words)\n",
        "            seq1.append(X1)\n",
        "            seq2.append(X2)\n",
        "            golds.append(score)\n",
        "        x1,m1 = prepare_data(seq1)\n",
        "        x2,m2 = prepare_data(seq2)\n",
        "        return x1,m1,x2,m2\n",
        "\n",
        "    prefix = \"../data/\"\n",
        "    farr = [\"MSRpar2012\"]\n",
        "    #farr = [\"MSRpar2012\",\n",
        "    #        \"MSRvid2012\",\n",
        "    #        \"OnWN2012\",\n",
        "    #        \"SMTeuro2012\",\n",
        "    #        \"SMTnews2012\", # 4\n",
        "    #        \"FNWN2013\",\n",
        "    #        \"OnWN2013\",\n",
        "    #        \"SMT2013\",\n",
        "    #        \"headline2013\", # 8\n",
        "    #        \"OnWN2014\",\n",
        "    #        \"deft-forum2014\",\n",
        "    #        \"deft-news2014\",\n",
        "    #        \"headline2014\",\n",
        "    #        \"images2014\",\n",
        "    #        \"tweet-news2014\", # 14\n",
        "    #        \"answer-forum2015\",\n",
        "    #        \"answer-student2015\",\n",
        "    #        \"belief2015\",\n",
        "    #        \"headline2015\",\n",
        "    #        \"images2015\",    # 19\n",
        "    #        \"sicktest\",\n",
        "    #        \"twitter\",\n",
        "    #        \"JHUppdb\",\n",
        "    #        \"anno-dev\",\n",
        "    #        \"anno-test\"]\n",
        "    (words, We) = getWordmap(wordfile)\n",
        "    df = np.zeros((len(words),))\n",
        "    dlen = 0\n",
        "    for f in farr:\n",
        "        g1x,g1mask,g2x,g2mask = getDataFromFile(prefix+f, words)\n",
        "        dlen += g1x.shape[0]\n",
        "        dlen += g2x.shape[0]\n",
        "        for i in xrange(g1x.shape[0]):\n",
        "            for j in xrange(g1x.shape[1]):\n",
        "                if g1mask[i,j] > 0:\n",
        "                    df[g1x[i,j]] += 1\n",
        "        for i in xrange(g2x.shape[0]):\n",
        "            for j in xrange(g2x.shape[1]):\n",
        "                if g2mask[i,j] > 0:\n",
        "                    df[g2x[i,j]] += 1\n",
        "\n",
        "    weight4ind = {}\n",
        "    for i in xrange(len(df)):\n",
        "        weight4ind[i] = np.log2((dlen+2.0)/(1.0+df[i]))\n",
        "    if save_file:\n",
        "        pickle.dump(weight4ind, open(save_file, 'w'))\n",
        "    return weight4ind"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/hackfinal/SIF/src/data_io.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnsEAc5IrgqJ",
        "outputId": "3d821532-fae0-4071-ea1e-62abc68ef193"
      },
      "source": [
        "%cd /content/hackfinal/SIF/src\n",
        "t1 = time.time()\n",
        "!python2 sif_embedding_my.py /content/hackfinal/words_en_vectors.txt /content/hackfinal/freqs_en.txt /content/hackfinal/sentences_en.txt > /content/hackfinal/embeddings_en.txt\n",
        "t2 = time.time()\n",
        "print(t2-t1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackfinal/SIF/src\n",
            "170.21436738967896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBXpY4eIA2tb",
        "outputId": "0367d925-a816-4861-953b-36d69582c0b4"
      },
      "source": [
        "%cd /content/hackfinal/SIF/src\n",
        "t1 = time.time()\n",
        "!python2 sif_embedding_my.py /content/hackfinal/words_ru_vectors.txt /content/hackfinal/freqs_ru.txt /content/hackfinal/sentences_ru.txt > /content/hackfinal/embeddings_ru.txt\n",
        "t2 = time.time()\n",
        "print(t2-t1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hackfinal/SIF/src\n",
            "112.51141357421875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA8PflCTBxtS"
      },
      "source": [
        "# convert obtained embeddings_*.txt to more compact format (2 embedding_*.npy files and 2 bundles_*.txt files)\n",
        "bundles_list_en = []\n",
        "vectors_en = []\n",
        "with open(\"/content/hackfinal/embeddings_en.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        spl = line.split(' ')\n",
        "        bundle = spl[0]\n",
        "        vec = np.array(list(map(float,spl[1:])))\n",
        "        bundles_list_en.append(bundle)\n",
        "        vectors_en.append(vec)\n",
        "vectors_en = np.array(vectors_en)\n",
        "\n",
        "bundles_list_ru = []\n",
        "vectors_ru = []\n",
        "with open(\"/content/hackfinal/embeddings_ru.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        spl = line.split(' ')\n",
        "        bundle = spl[0]\n",
        "        vec = np.array(list(map(float,spl[1:])))\n",
        "        bundles_list_ru.append(bundle)\n",
        "        vectors_ru.append(vec)\n",
        "vectors_ru = np.array(vectors_ru)\n",
        "\n",
        "with open(\"/content/hackfinal/bundles_ru.txt\", \"w\") as f:\n",
        "    for bundle in bundles_list_ru:\n",
        "        f.write(bundle+'\\n')\n",
        "\n",
        "with open(\"/content/hackfinal/bundles_en.txt\", \"w\") as f:\n",
        "    for bundle in bundles_list_en:\n",
        "        f.write(bundle+'\\n')\n",
        "\n",
        "with open('/content/hackfinal/embeddings_ru.npy', 'wb') as f:\n",
        "    np.save(f, vectors_ru)\n",
        "\n",
        "with open('/content/hackfinal/embeddings_en.npy', 'wb') as f:\n",
        "    np.save(f, vectors_en)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}